{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc3f07a4-4dfa-45b5-9bd5-ed6ef504c5b1",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f0d7b-5226-4d5b-bb31-bf0611e22bb8",
   "metadata": {},
   "source": [
    "We will be training a basic feedforward neural network using stochastic gradient descent.\n",
    "\n",
    "To do this we need to understand how backpropgation will work.\n",
    "\n",
    "We'll first need to understand what we are trying to minimize (our loss function). We will use a standard MSE error $(y-y_{pred})^2$\n",
    "\n",
    "Let's say we have a simple 3 layer Neural network, with 1 input layer, 1 output, and 1 hidden layer with 2 neurons.\n",
    "We can find the gradient w/ respect to the weights and biases through the chain rule\n",
    "\n",
    "Say we're trying to find the derivative of the loss w/ respect to the first weight connecting the input to the first neuron of the hidden layer. $\\frac{\\partial L}{\\partial w_1}$\n",
    "\n",
    "We thus have $\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y_{pred}} * \\frac{\\partial y_{pred}}{\\partial h_1} * \\frac{\\partial h_1}{\\partial w_1}$ through the chain rule where $h_1$ is the output of the first hidden node \n",
    "\n",
    "We therefore have $\\frac{\\partial L}{\\partial y_{pred}} = -2 * (y-y_{pred})$\n",
    "\n",
    "If $f$ is our activation function, we have $\\frac{\\partial y_{pred}}{\\partial h_1}= w_{h_1} * f'(w_{h_1}h_1+w_{h_2}h_2+b_{y_{pred}})$\n",
    "and\n",
    "$\\frac{\\partial h_1}{\\partial w_1}= \\text{input}_1 * f'(w_1\\text{input}_1+w_{2}\\text{input}_2+b_{h_1})$\n",
    "\n",
    "If we wanted for biases instead, we'd simply replace $\\frac{\\partial h_1}{\\partial w_1}$ with\n",
    "$\\frac{\\partial h_1}{\\partial b_1} = f'(w_1\\text{input}_1+w_{2}\\text{input}_2+b_{h_1})$\n",
    "\n",
    "If we assume a sigmoid activation function, these can be simplified using the derivative property of the function\n",
    "f'(x) = f(x)(1-f(x)) thus\n",
    "\n",
    "$\\frac{\\partial y_{pred}}{\\partial h_1}= w_{h_1} * y_{pred}(1-y_{pred})$ and\n",
    "$\\frac{\\partial h_1}{\\partial w_1}= \\text{input}_1 * h_1(1-h_1)$ and\n",
    "$\\frac{\\partial h_1}{\\partial b_1}= h_1(1-h_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31174644-9b61-4bc9-a63a-d8077a43fa53",
   "metadata": {},
   "source": [
    "# Doing it the long way\n",
    "First thing we'll do is try and code each weight and function individually. This is long and arduous and can likely be simplified through matrix multiplication but for now this is what we'll work with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604cf9e7-46da-41a2-8257-b0ca6801af33",
   "metadata": {},
   "source": [
    "![image](../img/StructureofNet.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540c9b81-f87b-454b-be25-331ccf24c777",
   "metadata": {},
   "source": [
    "## Defining Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7fd8c98-a4bb-434d-8164-47cb14ea9588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([ # First column is weight, second column is height\n",
    "  [-2, -1],  # Alice\n",
    "  [25, 6],   # Bob\n",
    "  [17, 4],   # Charlie\n",
    "  [-15, -6], # Diana\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "  1, # Alice\n",
    "  0, # Bob\n",
    "  0, # Charlie\n",
    "  1, # Diana\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c9c21-41f9-413a-93bb-63ee0ecd8190",
   "metadata": {},
   "source": [
    "## Initialize Weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34ce7afb-f1b3-4674-a5c9-5a5766efb542",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 2 * np.random.rand() - 1\n",
    "w2 = 2 * np.random.rand() - 1\n",
    "w3 = 2 * np.random.rand() - 1\n",
    "w4 = 2 * np.random.rand() - 1\n",
    "w5 = 2 * np.random.rand() - 1\n",
    "w6 = 2 * np.random.rand() - 1\n",
    "\n",
    "b1 = 2 * np.random.rand() - 1\n",
    "b2 = 2 * np.random.rand() - 1\n",
    "b3 = 2 * np.random.rand() - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2001e0-ebd2-4a69-9139-a5247c0b4749",
   "metadata": {},
   "source": [
    "## Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92c03164-7e5c-40ba-a377-4a82791cf2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(y, y_pred, deriv = False):\n",
    "    if(deriv):\n",
    "        return -2 * (y - y_pred)\n",
    "    return (y - y_pred) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d92d2a-611a-4d50-ab8e-f10eb4c962a0",
   "metadata": {},
   "source": [
    "## Define Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec160c24-f6da-4d78-a24d-4a9299c6612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(x, deriv = False):\n",
    "    if (deriv):\n",
    "        return x * (1-x)\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d01df-7618-4dd1-b3ef-29accf50ac4a",
   "metadata": {},
   "source": [
    "## Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45ec8d15-20b0-4102-85d8-9c9a0cc87878",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[0,:]\n",
    "y = all_y_trues[0]\n",
    "\n",
    "# Input - Hidden Layer\n",
    "h1 = Sigmoid(w1 * x[0] + w2 * x[1] + b1) \n",
    "h2 = Sigmoid(w3 * x[0] + w4 * x[1] + b2)\n",
    "\n",
    "# Hidden - Output Layer\n",
    "y_pred = Sigmoid(w5 * h1 + w6 * h2 + b3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c839398e-4620-47d0-843b-9017aeebbac5",
   "metadata": {},
   "source": [
    "## Calculate Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f04323dd-9ee6-44d5-b646-3f14c4d0c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = Loss(y, y_pred)\n",
    "\n",
    "# Derivatives\n",
    "\n",
    "## Output layer\n",
    "d_L_d_pred = Loss(y,y_pred, deriv = True)\n",
    "\n",
    "## Hidden Layer\n",
    "d_pred_d_h1 = w5 * Sigmoid(y_pred, deriv = True)\n",
    "d_pred_d_h2 = w6 * Sigmoid(y_pred, deriv = True)\n",
    "\n",
    "d_pred_d_w5 = h1 * Sigmoid(y_pred, deriv = True)\n",
    "d_pred_d_w6 = h2 * Sigmoid(y_pred, deriv = True)\n",
    "d_pred_d_b3 = Sigmoid(y_pred, deriv = True)\n",
    "\n",
    "## Input Layer\n",
    "d_h1_d_w1 = x[0] * Sigmoid(h1, deriv = True)\n",
    "d_h1_d_w2 = x[0] * Sigmoid(h1, deriv = True)\n",
    "d_h1_d_b1 = Sigmoid(h1, deriv = True)\n",
    "\n",
    "d_h2_d_w3 = x[0] * Sigmoid(h2, deriv = True)\n",
    "d_h2_d_w4 = x[0] * Sigmoid(h2, deriv = True)\n",
    "d_h2_d_b2 = Sigmoid(h2, deriv = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f44fd1-b076-45f8-977e-f48371184d2e",
   "metadata": {},
   "source": [
    "## Backpropagation update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3c15686-4fe6-441e-92a8-6553225be9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_update = d_L_d_pred * d_pred_d_h1 * d_h1_d_w1\n",
    "w2_update = d_L_d_pred * d_pred_d_h1 * d_h1_d_w1\n",
    "b1_update = d_L_d_pred * d_pred_d_h1 * d_h1_d_b1\n",
    "\n",
    "w3_update = d_L_d_pred * d_pred_d_h2 * d_h2_d_w3\n",
    "w4_update = d_L_d_pred * d_pred_d_h2 * d_h2_d_w4\n",
    "b2_update = d_L_d_pred * d_pred_d_h2 * d_h2_d_b2\n",
    "\n",
    "w5_update = d_L_d_pred * d_pred_d_w5\n",
    "w6_update = d_L_d_pred * d_pred_d_w6\n",
    "b3_update = d_L_d_pred * d_pred_d_b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaf78e7a-5ea2-424b-887e-59674c496cff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m w1 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mlearning_rate\u001b[49m \u001b[38;5;241m*\u001b[39m w1_update\n\u001b[0;32m      2\u001b[0m w2 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m w2_update\n\u001b[0;32m      3\u001b[0m w3 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m w3_update\n",
      "\u001b[1;31mNameError\u001b[0m: name 'learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "w1 -= learning_rate * w1_update\n",
    "w2 -= learning_rate * w2_update\n",
    "w3 -= learning_rate * w3_update\n",
    "w4 -= learning_rate * w4_update\n",
    "w5 -= learning_rate * w5_update\n",
    "w6 -= learning_rate * w6_update\n",
    "b1 -= learning_rate * b1_update\n",
    "b2 -= learning_rate * b2_update\n",
    "b3 -= learning_rate * b3_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a5050-9286-4899-b1b2-ab7c7d120f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
